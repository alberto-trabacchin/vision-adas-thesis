\chapter{Methods}

\section{Classical Computer Vision-based Approach}

This section is related to all the experiments done on the classical computer 
vision-based approach. The general scheme of the driver's attention model is 
shown in Figure \ref{fig:driver_attention}. 
The model is divided into four main stages: data synchronization, homography 
projection of the gaze, scene perception, and the driver's behavioral model. 

The data synchronization stage is responsible for aligning the data from the 
different sensors. In particular it is important that gaze data and 
images from the 
eye-tracking glasses are well synchronized each other and with video frames 
from the camera installed on the roof top of the car.

The homography projection of the gaze stage is responsible for projecting the 
gaze of the driver from the ETG camera plane to the roof top camera plane.
In this way we have a wider, more stable and accurate representation of 
the outside environment.
It is possible to 
estimate the homography transformation making the approximation that the matched 
keypoints are far away enough from the vehicle. This is a reasonable 
approximation since the driver is usually looking at the road and the objects 
far away from the vehicle. Moreover, the baseline of the stereo vision system is 
small compared to the depth of the keypoints.
The optimal way to estimate the projection would be through the epipolar 
geometry, estimating the fundamental matrix and the essential matrix. 
However, we have an uncalibrated stereo setup, and the baseline between the two 
cameras is not fixed. Even though it could be possible to integrate GPS data 
to estimate the two matrices, there is a consistent noise error that affects 
accuracy of the estimation.
The homography estimation is then divided in three steps: detection of keypoints 
in the two images through the SIFT algorithm, matching of the keypoints through 
RANSAC, and estimation of the homography matrix through the least squares method.

The scene perception stage is responsible for detecting and tracking the vulnerable 
users in the scene, such as pedestrians and cyclists. The detection is done 
through the YOLOv8 algorithm and the tracking through ByteTrack. In this way 
it is possible to compare the gaze of the driver with the state of the targets, 
including their position.

Finally, on the top, there is the driver's attention model. The responsibility 
of this block is to classify dangerous scenarios given the data from the 
driver and the scene. However, this block is sensitive to the quality of the 
signals of the previous stages.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{images/dreyeve/classic_scheme.png}
    \vspace*{0.6cm}
    \caption{The overall driver's attention scheme. It is divided into four main 
    stages: data synchronization, homography projection of the gaze, scene 
    perception and the driver's behavioral model.
    }
    \label{fig:driver_attention}
\end{figure}

\subsection{Homography Data Structure}
The Dr(eye)ve dataset is composed of 74 sequences of five minutes each. 
Moreover, the two cameras are not synchronized. 
The ETG camera has a frame rate of 30 fps, while the roof top camera has a 
frame rate of 25 fps. It is also important to notice that there are some videos 
that was recorded at slightly different frame rates. All the synchronization data 
are provided in the dataset.

Therefore, we decided to implement 
an interface to preprocess all the frames and synchronize them.
After the synchronization, we compute all the homographies and store them in a 
file. This file is then used to project the gaze of the driver in the roof top 
camera plane.
Furthermore, we chose to store other informations related to the quality of the 
estimation.
The data structure is described in Figure \ref{fig:homography_data_structure}:
it is a list of lists where each element is a unique sample that has the 
following fields: the gaze coordinates in the ETG and RT camera planes, the 
homography matrix, detected keypoints on the two planes, and the number of 
matchings between them.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/dreyeve/homography_data.png}
    \caption{The data structure used to store the homographies.
    \textbf{Left}: Each element is a specific sample in the i-th video, at 
    the j-th frame.
    \textbf{Right}: The attributes of each sample.}
    \label{fig:homography_data_structure}
\end{figure}

\subsection{Stereo Camera System Setup}
The cameras recording system can be reconducted to an uncalibrated stereo vision 
setup. In particular, the two cameras are not aligned and the baseline is not 
fixed. However, this is a problem for the projection of 3D world points between 
the two cameras. In fact, homography is a planar transformation, 
and it is possible to estimate the projection matrix if keypoints lie on the 
same plane.
For this reason we decided to make the approximation that the 
keypoints are far away enough from the vehicle. This is a reasonable 
approximation since the driver is usually looking at the road and the objects 
far away from the vehicle. Moreover, the baseline of the stereo vision system is 
small compared to the average depth of the keypoints.

In this way, it is not necessary to know intrinsic and extrinsic parameters of 
the cameras, such as focal length, resolution, and distortion coefficients. 
This approach is also proposed in the Dr(eye)ve paper \cite{dreyeve}.


\subsection{Targets Data Structure}
From the Dr(eye)ve dataset, we extracted the bounding boxes of the vulnerable 
road users, such as pedestrians and cyclists.
Moreover, through ByteTrack \cite{bytetrack}, we tracked the targets to have a 
spatio-temporal representation of the scene. Then, we compare the projected gaze 
of the driver with the location of targets. 

However, it is necessary to consider the quality of the tracking. Even though 
ByteTrack was specifically designed to track also overlapping objects, in driving 
scenarios it is not uncommon to have tracking failures. To partially mitigate 
the problem, we set two different states for each target: a 
\emph{detection} state and an \emph{observation} state.
\begin{itemize}
    \addtolength\itemsep{-2mm}
    \item \textbf{Detection}: the target is being detected and tracked by the algorithms.
    \item \textbf{Observation}: the target is being observed by the driver.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/dreyeve/targets_data.png}
\caption{The data structure used to store targets' states.
\textbf{Left}: Each element is a specific person, which contains the set of 
states during all its tracking.
\textbf{Right}: The attributes of the tracked person during time.
}
\label{fig:targets_data_structure}
\end{figure}
Through this approach, we can make sure if a person is both detected by the 
algorithm and observed by the driver. Without storing the detection data, there 
can be some unclear situations where the driver is looking at a target that is 
partially or completely occluded. That can happen because ByteTrack keeps the 
tracking of occluded targets through a Kalman filter. 
The data structure is represented in details 
in Figure \ref{fig:targets_data_structure}: it is a list of lists where each 
element is a unique person that has been tracked in the respective video.

Each person has the following list of states: a unique tracking ID, the 
detection states, the observation states, the bounding boxes' coordinates with 
the confidence score, and the coordinates of the projected driver's gaze.

\subsection{Adding Depth Information}
Targets' depth information can be very informative for the driver's attention 
model. In fact, the driver is usually more interested in the objects that are 
closer to the vehicle.
Considering the cameras' setup there are three main ways to include depth 
information of the scene: using a neural network to estimate the depth of the 
detected objects from the bounding box dimensions, leveraging the stereo vision 
data and car's location information (GPS and speed) to estimate the depth of 
similar keypoints in consecutive timeframes, or to compute a monocular depth 
map through a pretrained model.

The first approach is the simplest and the fastest, but it is also the less 
accurate. In fact the depth of an object is not only related to its dimensions, 
especially for people. Bounding boxes can vary depending on the target age, pose, 
occlusions and if they are riding a vehicle, like a bicycle. 
Moreover, a custom calibration should be made at least for each camera model, 
with its dedicated lens and image sensor. Image distortion could heavily affect 
the quality of predictions. However, this is not feasible in our case because 
we do not have any ground truth information to fine-tune the model.
Some recent works on targets' depth estimation with stereo vision systems were 
proposed in \cite{li2019stereo}, \cite{Peng_2020_CVPR}. There are also studies 
on retrieving the depth of objects from monocular images \cite{bbox_mde}.

The second approach is the most accurate and robust, but it requires an accurate 
calibration of the stereo vision system to perform well. Moreover, data depth 
is only related to correspondent keypoints. This means that if no matching 
keypoints related to a target are found, it is not possible to estimate its 
depth. Finally, the stereo vision system is not always reliable, especially when 
it is uncalibrated and the baseline is not fixed and relatively small compared to 
the average depth of the keypoints. 
Recently some self-calibration methods have been proposed 
\cite{sfm_self_calibration1}, \cite{sfm_self_calibration2}.
The roof top camera has a wide field of 
view because it uses a wide angle lens, and then an anti-distortion algorithm 
is computed. This affects the quality of the depth estimation.

The third approach is the most versatile and suitable to the specific application.
In fact, the dense depth map computed by the pretrained model is not related to the 
ETG camera and allows to estimate the depth of all the objects in the scene.
The model is trained on a large dataset and is able to generalize well to 
different scenarios. The only drawback is that the depth map is not absolute. 
This means that it is not possible to estimate the distance of objects in meters, 
but through a relative scale depending on the maximum and minimum depths of the 
scene. However, this should not be able to compromise the quality of the 
driver's attention model because that is the same approach we use as humans 
when driving. However, we actually compute a hybrid approach where we use a sort 
of stereo vision system through our eyes and our knowledge and past experience 
to make an approximate estimation of the depth of the objects.
In the experiments section we will show the performance of pretrained MiDaS 
\cite{midas}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/dreyeve/depth_estimation.png}
\caption{Summary of the possible three methods to estimate the depth of targets.}
\label{fig:depth_estimation}
\end{figure}



\subsection{Adding Spatial Information}
Spatial information can be fundamental to analyze the driver's attention 
towards targets in the scene. In fact, the driver usually pays more attention 
to some locations of the field of view, depending on the context (e.g. type of 
road, traffic, weather conditions, crowdness, etc.).
Moreover, the double camera setup allows to have a wider field of view on the 
rooftop camera. This is particularly useful to track the gaze also when the 
driver is moving the head. 

Therefore, an initial approach is to include the spatial information of the 
gaze in the driver's attention model. In particular, we divided the roof top 
camera view in a 3x7 grid, as shown in Figure \ref{fig:rt_camera_grid}. 
This division is made to have a more detailed representation of the scene, in 
fact it is possible to notice that cells from  1 to 7 are out of interest when 
the driver is looking straight ahead. These area are 
located on the top of the image, where the driver is usually not looking at, 
except for some specific situations (e.g. traffic lights, road signs, etc.). 

Cells 8, 9, 13, 14 represent out of field of view areas, where there could be 
potential threats (e.g. a pedestrian crossing the road from the left side, 
a car crossing the road). 

Cells 10, 11, 12 are the center of the field of view, 
where the driver is usually looking at. In particular, considering the city 
center shown in Figure \ref{fig:rt_camera_grid}, area 11 captures front 
vehicles. 
Cells 10, 12, show possible targets interacting 
in the near future with the ego-vehicle 
from the road sides (e.g. the ego-vehicle approaching a crosswalk with a 
pedestrian waiting to cross the road).

Cells 15, 16, 20, 21 represent the left and sie corners out of the field of view 
of the ETG camera. These areas are similar to cells 8, 9, 13, 14, but on with 
closer targets. Therefore they are more dangerous because targets on these 
locations are more challenging to spot and the driver has less time to react.

Cell 17, 18, 19 are at the bottom-center of the field of view.
These areas cover in part the dashboard of the car, and the driver is usually 
looking at them when checking the speed, the fuel, the GPS, etc. 
Depending on the context, these areas could also cover close vehicles on the 
front of the ego-vehicle (e.g. a line of vehicles at a traffic light). 
Therefore, these areas can identify a warning when the driver is not 
looking at the road and there is a potential threat.

In general, we identified different areas of interest in the field of view of 
the driver, depending on the specific context. This is an initial equal division 
that still helps characterize the scene. However, considering the wide view of 
the roof top camera, and the distortion applied to the image, there are better 
solutions to divide the scene. For example, it is possible to make a non-equal 
division, grouping together areas that are more similar to each other.
In summary, spatial information is both informative and complicated to manage 
because it is highly dependent on the context. Therefore, it could be helpful 
to let a model learn the spatial information by itself, through a deep learning 
approach. This mainly motivates the move from the classical computer 
vision-based approach to the deep learning-based approach.
 
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/dreyeve/rt_cam_grid.png}
\caption{Division of the roof top camera view in a 3x7 grid.}
\label{fig:rt_camera_grid}
\end{figure}


\section{Deep Learning-based Approach}
In this section we describe the deep learning-based approach to the driver's 
attention model.
Even though in the previous section we identified the main stages of the 
classical computer vision-based approach, and many positive aspects were 
highlighted, there are also some drawbacks.
In particular, for each stage there are some simplification hypothesis that 
affect quality of inputs for the driver's attention model.

For example, the homography projection of the gaze is not perfect, and the 
approximation that the matched keypoints are far away enough from the vehicle 
is not always true. Moreover, the quality of homography estimation heavily 
depends on the quality of the keypoints' detection and matching. This can be 
compromised by the quality of the images, presence of occlusions, light and 
weather conditions, etc.

The scene perception stage is also affected by the quality of the detection 
and tracking algorithms. In particular, ByteTrack is not always able to track
overlapping objects, and the quality of the tracking is affected by the 
quality of the detection. Moreover, the detection is not always perfect, and 
there are some false positives and false negatives. 

Depth estimation stage is also affected by the quality of the pretrained 
model of MiDaS. In particular, the model is trained on a large datasets in 
completely different contexts, and it is not always able to generalize well to 
different scenarios. Moreover, the depth map is not absolute, and it is not 
possible to estimate the absoulte distance in meters.

That is why it is better to let a model learn important features by itself, through 
some classification biases used to label a dataset. We decided to use a vision 
transformer model to learn the self-attention map of the scene to detect 
potential threats. 

\subsection{Attention Map for Anomaly Detection}
The vision transformer model is able to learn the self-attention map of each 
frame in the scene.


\newlength{\subfigwidth}
\setlength{\subfigwidth}{32mm}
\newlength{\horspace}
\setlength{\horspace}{.25\textwidth}
\begin{figure}
    \caption{Attention masks for a pre-trained multi-head ViT \cite{attention_vit}.}
    \centering
    \begin{tabular}{r p{\horspace} p{\horspace} p{\horspace}}
    Original & 
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/1/img.png}
    \end{subfigure}
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/2/img.png}
    \end{subfigure} 
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/4/img.png}
    \end{subfigure} \\
    %
    Head 1 &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/1/attn-head0.png}
    \end{subfigure}
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/2/attn-head0.png}
    \end{subfigure} 
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/4/attn-head0.png}
    \end{subfigure} \\
    %
    Head 2 &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/1/attn-head1.png}
    \end{subfigure}
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/2/attn-head1.png}
    \end{subfigure} 
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/4/attn-head1.png}
    \end{subfigure} \\
    Head 3 &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/1/attn-head2.png}
    \end{subfigure}
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/2/attn-head2.png}
    \end{subfigure} 
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/4/attn-head2.png}
    \end{subfigure} \\
    Head 4 &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/1/attn-head3.png}
    \end{subfigure}
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/2/attn-head3.png}
    \end{subfigure} 
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/4/attn-head3.png}
    \end{subfigure} \\
    Head 5 &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/1/attn-head4.png}
    \end{subfigure}
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/2/attn-head4.png}
    \end{subfigure} 
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/4/attn-head4.png}
    \end{subfigure} \\
    Head 6 &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/1/attn-head5.png}
    \end{subfigure}
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/2/attn-head5.png}
    \end{subfigure} 
    \hfill &
    \begin{subfigure}[b]{\subfigwidth}
        \includegraphics[width=\subfigwidth]{images/vit_attention/4/attn-head5.png}
    \end{subfigure} \\
\end{tabular}
\end{figure}
    
    % Image & 
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{I32}
    %     \includegraphics[width=\subfigwidth]{example-image-a}
    % \end{subfigure} &
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{TV}
    %     \includegraphics[width=\subfigwidth]{example-image-a}
    % \end{subfigure} &
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{DIP}
    %     \includegraphics[width=\subfigwidth]{example-image-a}
    % \end{subfigure} &
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{sst}
    %     \includegraphics[width=\subfigwidth]{example-image-a}
    % \end{subfigure} \\
    
    % Residual & 
    % \includegraphics[width=\subfigwidth]{example-image-b} &
    % \includegraphics[width=\subfigwidth]{example-image-b} &
    % \includegraphics[width=\subfigwidth]{example-image-b} &
    % \includegraphics[width=\subfigwidth]{example-image-b}
    
    % \end{tabular}
    
    % \begin{tabular}{r p{\subfigwidth} p{\subfigwidth} p{\subfigwidth} p{\subfigwidth}}
    
    % Image & 
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{gst}
    %     \includegraphics[width=\subfigwidth]{example-image-c}
    % \end{subfigure} &
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{est}
    %     \includegraphics[width=\subfigwidth]{example-image-c}
    % \end{subfigure} &
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{mst}
    %     \includegraphics[width=\subfigwidth]{example-image-c}
    % \end{subfigure} &
    % \begin{subfigure}[b]{\subfigwidth}
    %     \caption{est}
    %     \includegraphics[width=\subfigwidth]{example-image-c}
    % \end{subfigure} \\
    
    % Residual & 
    % \includegraphics[width=\subfigwidth]{example-image-a} &
    % \includegraphics[width=\subfigwidth]{example-image-a} &
    % \includegraphics[width=\subfigwidth]{example-image-a} &
    % \includegraphics[width=\subfigwidth]{example-image-a}
    
    % \end{tabular}
    % \end{figure}


\subsection{The Artificial Bias}

\subsection{The Artificial Bias}

\subsection{Data Preprocessing of Dr(eye)ve}

\subsection{Data Preprocessing of BDD100k}


\subsection{Handling Unbalanced Data}

\subsubsection{Evaluation Metrics}

\subsubsection{Data Augmentation}

% \subsubsection{Upsampling}

% \subsubsection{Downsampling}	

\subsubsection{Weighting Loss Function}

% \subsubsection{Synthetic Minority Oversampling Technique (SMOTE)}

% \subsection{Receiver Operating Characteristic (ROC) Curve}