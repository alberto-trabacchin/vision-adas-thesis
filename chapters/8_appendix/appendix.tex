\begin{appendices}
    \chapter{Training Algorithms}
    \label{appendix:training_algorithms}
    \begin{algorithm}
        \nextfloat{\caption{Preprocessing of DR(eye)VE dataset}}
        \vspace{0.2cm}
        \DontPrintSemicolon
        \textit{trainSet, valSet, testSet, ulSet} $\gets$ \textbf{Dreyeve}(\textit{splitIdxs, resolution, fps})\;
        \textit{validator} $\gets$ \textbf{RetinaNet}(\,)\;
        \For{\textit{set} $\gets$ \{\textit{trainSet, valSet, testSet}\}}{
            \textit{labels} $\gets$ validator(\textit{set})\;
            \textit{set.labels} $\gets$ \textit{labels}\;
        }
        \textbf{save} \textit{trainSet, valSet, testSet, ulSet}\;
    \end{algorithm}
    \begin{algorithm}
        \nextfloat{\caption{Semi-supervised training with Meta-Pseudo Labels}}
        \vspace{0.2cm}
        \DontPrintSemicolon
        \textit{trainSet, valSet, testSet, ulSet} $\gets$ \textbf{load} data\;
        \textit{teacher} $\gets$ \textbf{ViT}(\textit{patch, depth, embedding, mlp, heads})\;
        \textit{student} $\gets$ \textbf{ViT}(\textit{patch, depth, embedding, mlp, heads})\;
        \While{$\text{training}$}{
            \textit{pseudoLabels} $\gets$ \textit{teacher}(\textit{ulSet})\;
            \text{train(\textit{student, ulSet, pseudoLabels})}\;
            \text{train(\textit{student, trainSet})}\;
            \text{train(\textit{teacher, trainSet})}\;
            \textit{S.metrics} $\gets$ \text{validate(\textit{student, valSet})}\;
            \textit{T.metrics} $\gets$ \text{validate(\textit{teacher, valSet})}\;
            \If {$\textit{S.metrics.accuracy}$ \text{is increased}}{
                \textbf{save} \textit{student}\;
            }
        }
    \end{algorithm}
    \begin{algorithm}
        \nextfloat{\caption{Update training set with $N$ wrong predictions}}
        \vspace{0.2cm}
        \DontPrintSemicolon
        \textit{trainSet} $\gets$ \textbf{load} \text{training set}\;
        \textit{model} $\gets$ \textbf{load} \text{trained model}\;
        \textit{validator} $\gets$ \textbf{RetinaNet}(\,)\;
        \textit{S.count, S.max} $\gets$ $0, \,N/2$\;
        \textit{D.count, D.max} $\gets$ $0, \,N/2$\;
        \For{sample $\gets$ \textit{ulSet}}{
            \If{(\textit{S.count = S.max}) \textbf{and} (\textit{D.count = D.max})}{
                \textbf{break}\;
            }
            \textit{label} $\gets$ \textit{validator}(\textit{sample})\;
            \textit{pred} $\gets$ \textit{model}(\textit{sample})\;
            \If{\textit{pred} $=$ \textit{label}}{
                \textbf{continue}\;
            }
            \If{(\textit{label=Safe}) \textbf{and} (\textit{S.count \textless S.max})}{
                \textit{trainSet}.append(\textit{sample, label})\;
                \textit{S.count} $\gets$ \textit{S.count} $+1$\;
            }
            \If{(\textit{label=Dangerous}) \textbf{and} (\textit{D.count \textless D.max})}{
                \textit{trainSet}.append(\textit{sample, label})\;
                \textit{D.count} $\gets$ \textit{D.count} $+1$\;
            }
            \If{\textbf{endof} \textit{ulSet}}{
                \textbf{break}\;
            }
        }
        \textbf{save} \textit{trainSet}\;
    \end{algorithm}

\chapter{Probabilistic Perspective of \ac{auc}}
\label{appendix:auc_probabilistic}
In this appendix, we provide a more formal and detailed explanation of the \ac{auc}-\ac{roc}
from a probabilistic perspective. It is fundamental to better interpret results on 
unbalanced datasets and to understand the trade-off between \ac{tpr} and \ac{fpr}.
We took inspiration from the work of David J. Hand \cite{roc_auc_probabilistic} 
and adapted the explanation to our specific case.

\section{Definitions}
A trained model for binary classification can also be considered as a probabilistic 
classifier that outputs a score $s(\boldsymbol{x}) = p(\text{class}=1\,|\,\boldsymbol{x})$,
where $\boldsymbol{x}$ is the input data.
Considering the set of classes $k = \{0, 1\}$, $f_k(s)$ is the \ac{pdf} of the 
score $s$ for class $k$. Its \ac{cdf} is defined as $F_k(s)$.
Therefore \ac{fpr} and \ac{tpr} can be defined as:
\begin{align*}
    \text{\ac{fpr}} &= 1 - F_0(s) = 1 - P(\boldsymbol{X_0} \leq s) = P(\boldsymbol{X_0} > s)\\
    \text{\ac{tpr}} &= 1 - F_1(s) = 1 - P(\boldsymbol{X_1} \leq s) = P(\boldsymbol{X_1} > s)
\end{align*}
where $\boldsymbol{X_0}$ and $\boldsymbol{X_1}$ are random variables representing 
the score $s$ for a random sample of class 0 and 1, respectively.
For simplicity, \ac{fpr} and \ac{tpr} are respectively defined as $x(s)$, $y(s)$,
and the decision threshold is $\tau$. Moreover, from the definition of \ac{roc} curve,
we have $y(x(\tau)) = y(\tau)$.
 Then we have:
\begin{align*}
    \ac{fpr} &= x(s) \\
    \ac{tpr} &= y(s)
\end{align*}

\section{Derivation}
Considering that the \ac{auc} is the area under the \ac{roc} curve, we can express 
it as:
\begin{align}
    \text{\ac{auc}} &= \int_{0}^{1} y(x)dx \nonumber\\
    &= \int_{0}^{1} y(x(\tau))dx(\tau) \nonumber
\end{align}
Changing the variable of integration from $x$ to $\tau$, and knowing that 
$dx(\tau) = x'(\tau)d\tau$:
\begin{align}
    \text{\ac{auc}} &= \int_{+\infty}^{-\infty} y(\tau)x'(\tau)d\tau \nonumber\\
    &= \int_{+\infty}^{-\infty} (1-F_1(\tau)) (-f_0(\tau)) d\tau \nonumber \\
    &= \int_{-\infty}^{+\infty} (1 - F_1(\tau))f_0(\tau) d\tau 
    \label{eq:auc1}\\
    &= \int_{-\infty}^{+\infty} P(\boldsymbol{X_1} > \tau) P(\boldsymbol{X_0} = \tau) d\tau
    \label{eq:auc2}
\end{align}

\section{Interpretation}
From Equation \ref{eq:auc1}, we can interpret the \ac{auc} as the probability that 
a randomly chosen positive sample has a higher score than a randomly chosen negative 
sample. This is equivalent to multiplying the \ac{cdf} of the positive class by the 
\ac{pdf} of the negative class and integrating over all possible values of the score.
A more intuitive notation is also given in Equation \ref{eq:auc2}.

Considering our problem of detecting dangerous events in driving scenes, the 
\ac{auc} can be interpreted as the probability that a randomly chosen dangerous 
case has a higher score than a randomly chosen safe case. In other words, the 
probability that the model does not misclassify a dangerous event as safe. 
This is a fundamental aspect to consider when evaluating the performance of 
a model on unbalanced datasets. Moreover, we want to weight more false negatives 
than false positives, as the consequences of missing a dangerous event are more 
severe than misclassifying a safe event. Therefore, the \ac{auc} is a good metric 
to evaluate the performance of a model in this context, defining dangerous events 
as the positive class.

\end{appendices}