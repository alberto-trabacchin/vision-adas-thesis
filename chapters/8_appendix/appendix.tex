\begin{appendices}
    \chapter{Training Algorithms}
    \label{appendix:training_algorithms}
    \section{Pre-processing of DR(eye)VE dataset}
    Algorithm \ref{alg:preprocessing} describes how the DR(eye)VE dataset is 
    pre-processed before training the model. The dataset is split into training, 
    validation, test, and unlabelled sets. The \textbf{Dreyeve} data structure 
    accepts the split indexes, resolution, and frames per second as input parameters, 
    where split indexes are used to divide the dataset into the sets.

    A validator model is used to label the training, validation, and test sets. 
    The validator is a pre-trained \textbf{RetinaNet} model. Labels are generated 
    depending on the objects detected by the validator. Finally, all the sets are 
    saved to disk for further training.
    \begin{algorithm}
        \nextfloat{\caption{Preprocessing of DR(eye)VE dataset}\label{alg:preprocessing}}
        \vspace{0.2cm}
        \DontPrintSemicolon
        \textit{trainSet, valSet, testSet, ulSet} $\gets$ \textbf{Dreyeve}(\textit{splitIdxs, resolution, fps})\;
        \textit{validator} $\gets$ \textbf{RetinaNet}(\,)\;
        \For{\textit{set} $\gets$ \{\textit{trainSet, valSet, testSet}\}}{
            \textit{labels} $\gets$ validator(\textit{set})\;
            \textit{set.labels} $\gets$ \textit{labels}\;
        }
        \textbf{save} \textit{trainSet, valSet, testSet, ulSet}\;
    \end{algorithm}
    \section{Pseudo-Labelling}
    Algorithm \ref{alg:pseudo_labelling} describes the semi-supervised training 
    with Meta-Pseudo Labels \cite{pham2021meta}.
    Splits of the dataset are loaded, and two models are initialized: a teacher 
    and a student. The models are based on a vision transformer architecture, 
    represented by the \textbf{ViT} model. Most important hyperparameters that 
    need to be defined are the patch size, depth of the model, embedding size, 
    dimension of the multi-layer perceptron layer, and number of attention heads.

    The training loop is executed until convergence of the validation accuracy 
    of the student model. In each iteration, in order, the teacher generates 
    pseudo-labels on the unlabelled set, and the student is trained with the 
    pseudo-labels on the unlabelled set. Then, both the student and teacher 
    are trained on the training set. The student model, that is supposed to 
    perform better than the teacher, is saved if the validation accuracy is 
    increased.
    \begin{algorithm}
        \nextfloat{\caption{Semi-supervised training with Meta-Pseudo Labels}
        \label{alg:pseudo_labelling}}
        \vspace{0.2cm}
        \DontPrintSemicolon
        \textit{trainSet, valSet, testSet, ulSet} $\gets$ \textbf{load} data\;
        \textit{teacher} $\gets$ \textbf{ViT}(\textit{patch, depth, embedding, mlp, heads})\;
        \textit{student} $\gets$ \textbf{ViT}(\textit{patch, depth, embedding, mlp, heads})\;
        \While{$\text{training}$}{
            \textit{pseudoLabels} $\gets$ \textit{teacher}(\textit{ulSet})\;
            \text{train(\textit{student, ulSet, pseudoLabels})}\;
            \text{train(\textit{student, trainSet})}\;
            \text{train(\textit{teacher, trainSet})}\;
            \textit{S.metrics} $\gets$ \text{validate(\textit{student, valSet})}\;
            \textit{T.metrics} $\gets$ \text{validate(\textit{teacher, valSet})}\;
            \If {$\textit{S.metrics.accuracy}$ \text{is increased}}{
                \textbf{save} \textit{student}\;
            }
        }
    \end{algorithm}
    \section{Correction with Wrong Predictions}
    Algorithm \ref{alg:dataset_update} describes the process of updating the 
    training set of DR(eye)VE to correct the model on the wrong predictions.
    The training set and the trained model are loaded, and a validator model 
    \textbf{RetinaNet} is initialized. 
    
    The algorithm iterates over the unlabelled 
    set, and compares the predictions of the model with the labels provided by 
    the validator. If the prediction is wrong, the sample is added to the training 
    set and the respective counter is increased. The process is repeated until 
    the maximum number of wrong predictions is reached for each class.

    This process also helps to rebalance the dataset during training. 
    However, if the there are not enough wrong predictions to update the dataset, 
    the algorithm stops after iterating over the unlabelled set. Therefore, an 
    unbalanced quantity of new labels is added to the training set.
    \begin{algorithm}
        \nextfloat{\caption{Update training set with $N$ wrong predictions}
        \label{alg:dataset_update}}
        \vspace{0.2cm}
        \DontPrintSemicolon
        \textit{trainSet} $\gets$ \textbf{load} \text{training set}\;
        \textit{model} $\gets$ \textbf{load} \text{trained model}\;
        \textit{validator} $\gets$ \textbf{RetinaNet}(\,)\;
        \textit{S.count, S.max} $\gets$ $0, \,N/2$\;
        \textit{D.count, D.max} $\gets$ $0, \,N/2$\;
        \For{sample $\gets$ \textit{ulSet}}{
            \If{(\textit{S.count = S.max}) \textbf{and} (\textit{D.count = D.max})}{
                \textbf{break}\;
            }
            \textit{label} $\gets$ \textit{validator}(\textit{sample})\;
            \textit{pred} $\gets$ \textit{model}(\textit{sample})\;
            \If{\textit{pred} $=$ \textit{label}}{
                \textbf{continue}\;
            }
            \If{(\textit{label=Safe}) \textbf{and} (\textit{S.count \textless S.max})}{
                \textit{trainSet}.append(\textit{sample, label})\;
                \textit{S.count} $\gets$ \textit{S.count} $+1$\;
            }
            \If{(\textit{label=Dangerous}) \textbf{and} (\textit{D.count \textless D.max})}{
                \textit{trainSet}.append(\textit{sample, label})\;
                \textit{D.count} $\gets$ \textit{D.count} $+1$\;
            }
            \If{\textbf{endof} \textit{ulSet}}{
                \textbf{break}\;
            }
        }
        \textbf{save} \textit{trainSet}\;
    \end{algorithm}

\chapter{Probabilistic Perspective of \acs{auc}}
\label{appendix:auc_probabilistic}
In this appendix, we provide a more formal and detailed explanation of the \ac{auc}-\ac{roc}
from a probabilistic perspective. It is fundamental to better interpret results on 
unbalanced datasets and to understand the trade-off between \ac{tpr} and \ac{fpr}.
We took inspiration from the work of David J. Hand \cite{roc_auc_probabilistic} 
and adapted the explanation to our specific case.

\section{Definitions}
A trained model for binary classification can also be considered as a probabilistic 
classifier that outputs a score $s(\boldsymbol{x}) = p(\text{class}=1\,|\,\boldsymbol{x})$,
where $\boldsymbol{x}$ is the input data.
Considering the set of classes $k = \{0, 1\}$, $f_k(s)$ is the \ac{pdf} of the 
score $s$ for class $k$. Its \ac{cdf} is defined as $F_k(s)$.
Therefore \ac{fpr} and \ac{tpr} can be defined as:
\begin{align*}
    \text{\ac{fpr}} &= 1 - F_0(s) = 1 - P(\boldsymbol{X_0} \leq s) = P(\boldsymbol{X_0} > s)\\
    \text{\ac{tpr}} &= 1 - F_1(s) = 1 - P(\boldsymbol{X_1} \leq s) = P(\boldsymbol{X_1} > s)
\end{align*}
where $\boldsymbol{X_0}$ and $\boldsymbol{X_1}$ are random variables representing 
the score $s$ for a random sample of class 0 and 1, respectively.
For simplicity, \ac{fpr} and \ac{tpr} are respectively defined as $x(s)$, $y(s)$,
and the decision threshold is $\tau$. Moreover, from the definition of \ac{roc} curve,
we have $y(x(\tau)) = y(\tau)$.
 Then we have:
\begin{align*}
    \ac{fpr} &= x(s) \\
    \ac{tpr} &= y(s)
\end{align*}

\section{Derivation}
Considering that the \ac{auc} is the area under the \ac{roc} curve, we can express 
it as:
\begin{align}
    \text{\ac{auc}} &= \int_{0}^{1} y(x)dx \nonumber\\
    &= \int_{0}^{1} y(x(\tau))dx(\tau) \nonumber
\end{align}
Changing the variable of integration from $x$ to $\tau$, and knowing that 
$dx(\tau) = x'(\tau)d\tau$:
\begin{align}
    \text{\ac{auc}} &= \int_{+\infty}^{-\infty} y(\tau)x'(\tau)d\tau \nonumber\\
    &= \int_{+\infty}^{-\infty} (1-F_1(\tau)) (-f_0(\tau)) d\tau \nonumber \\
    &= \int_{-\infty}^{+\infty} (1 - F_1(\tau))f_0(\tau) d\tau 
    \label{eq:auc1}\\
    &= \int_{-\infty}^{+\infty} P(\boldsymbol{X_1} > \tau) P(\boldsymbol{X_0} = \tau) d\tau
    \label{eq:auc2}
\end{align}

\section{Interpretation}
From Equation \ref{eq:auc1}, we can interpret the \ac{auc} as the probability that 
a randomly chosen positive sample has a higher score than a randomly chosen negative 
sample. This is equivalent to multiplying the \ac{cdf} of the positive class by the 
\ac{pdf} of the negative class and integrating over all possible values of the score.
A more intuitive notation is also given in Equation \ref{eq:auc2}.

Considering our problem of detecting dangerous events in driving scenes, the 
\ac{auc} can be interpreted as the probability that a randomly chosen dangerous 
case has a higher score than a randomly chosen safe case. In other words, the 
probability that the model does not misclassify a dangerous event as safe. 
This is a fundamental aspect to consider when evaluating the performance of 
a model on unbalanced datasets. Moreover, we want to weight more false negatives 
than false positives, as the consequences of missing a dangerous event are more 
severe than misclassifying a safe event. Therefore, the \ac{auc} is a good metric 
to evaluate the performance of a model in this context, defining dangerous events 
as the positive class.

\end{appendices}