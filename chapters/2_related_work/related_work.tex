\chapter{Related Work}
\label{chpt:related_work}
We review recent work in behavioral intention prediction through standard 
methods, including object detection and tracking, and more advanced deep learning 
techniques. Moreover, we introduce advantages and limitations of some methods 
that are capable to improve scene understanding.

\section{Available Datasets}
Pedestrian detection and tracking are fundamental tasks in the field of computer 
vision applied to advanced driving assistance systems. If this task is innate 
to humans, it is still a challenging problem for machines. In addition to the 
complexity of the task many applications require real-time processing 
constraints which can affect accuracy of results. 

In the computer vision field object detection can be approached through two f
different perspectives. The former is to make predictions on general datasets 
that are used as benchmarks for the community, such as COCO \cite{coco}.
In this case the goal is to develop a vision model capable to detect a wide 
range of objects, in many different scenarios.
The latter is task-specific oriented, where the goal is to detect a relative 
small subset of objects in a specific context. This is the case of pedestrian 
detection in driving scenarios.

Nowadays, in the field of intelligent transportation systems, many datasets 
have been released to accomplish detection of some common objects, including 
pedestrians, cyclists, and different types of vehicles.
The most common used for benchmarking state of the art algorithms are KITTI 
\cite{kitti}, Berkeley DeepDrive \cite{bdd100k}, Waymo Open Dataset 
\cite{waymo} and nuScenes \cite{nuscenes}. Moreover, each of these datasets 
has its own peculiarity, such as the quantity of video recordings, the variety 
of scenarios, the number of annotated objects, and the quality of ground-truth 
targets, that can be manually annotated or automatically generated.

Despite semi-autonomous driving is getting a lot of attention in the last years 
and it is expected to continue to grow, the state of the art algorithms are far 
away from the robustness and reliability that industry requires. Moreover, it 
has been shown that the performance can drastically drop when models are tested 
on different datasets, and some customized evaluation metrics could be required 
depending on the accomplished task \cite{uricar2019challenges}.
Therefore it is fundamental to deserve as much attention to the quantity and quality of the data 
as to model design process.

\section {Multi-Object Tracking}
Multi-object tracking (MOT) aims to estimate bounding boxes of objects and 
associate them an unique ID across multiple time frames. This task is 
fundamental in perception of the environment because human behaviors are often 
time-dependent. For example it is important to predict pedestrians' trajectories 
to avoid collisions or to understand their intentions. Tracking is performed by 
estimating the future position of objects based on their past spatial and 
temporal information.

Object detection and tracking are distinct tasks, yet both are essential for 
the tracking process. Within tracking, there are two main aims: 
\textit{"tracking by detection"} and \textit{"detection by tracking"}. 
The former leverages powerful modern object detectors to increase tracking 
accuracy. Usually only bounding boxes' features are considered for the task.
The latter, on the other hand, aims to imporve tracking accuracy with the help 
of tracking information. After the detection phase, most MOT methods discard 
some bounding boxes depending on a detection threshold 
\cite{retinatrack,transtrack}; 
therefore all objects related to these boxes are lost for the considered time 
frame. There are also other methods, like ByteTrack \cite{bytetrack}, that are 
able to track partially-occluded objects in real-world scenarios with 
real-time performance. 

For example, ByteTrack does not discard all bounding 
boxes below a certain confidence threshold, but it keeps almost every detection 
dividing them into high score ones and low score ones. At first high score 
detection boxes are matched with corresponding tracklets. Then, due to 
occlusions, resizes and motion blur, remaining unmatched tracklets are 
combined with low score detection boxes. 

\section{Image Classification}
Image classification is a fundamental task in computer vision, as it is also 
used in the detection pipeline. In the last years, many models based on 
convolutional neural networks (CNNs) have been proposed, but nowadays research 
is moving towards more complex classfication tasks using different 
architectures.
The most succesful CNN-based models are, in a chronological order, 
AlexNet \cite{alexnet}, 
VGG \cite{vggnet}, Inception \cite{inception}, ResNet \cite{resnet}, 
DenseNet \cite{densenet} and 
EfficientNet \cite{efficientnet}.

AlexNet \cite{alexnet} was among the pioneering models to incorporate deep convolutional 
layers, stacking five layers that increased the parameter count to 60 million. 
Additionally, it implemented several strategies to mitigate overfitting, 
including the application of dropout. 
Then VGG \cite{vggnet} still increased the number of 
convolutional layers to 16, reaching the state of the art in the ImageNet 
dataset \cite{imagenet}. However, its 138 million parameters made it 
computationally expensive.
Inception \cite{inception}, on the other hand, was able to increase 
the accuracy reducing the model size through multi-scale convolution transforms 
with filters of varying sizes. Transformations were applied to input features 
in parallel, and then concatenated to form the output.
Therefore, it was able to capture diverse spatial information reducing the 
model size to 4 million parameters.
Then ResNet \cite{resnet} brought a revolutional change of CNN architectures, introducing 
the concept of \textit{residual learning}. In particular it addressed the 
problem of vanishing and exploding gradients in very deep models by introducing 
skip connections between layers. This allowed to train a 152-layer CNN model 
that outperformed shallower models.
After ResNet, DenseNet \cite{densenet} introduced another novel aspect by fully connecting 
all convolutional layers with each other in a forward way. 
Unlike traditional CNN architectures where layers are connected in a sequential 
manner, DenseNet establishes direct connections between all layers in a 
feed-forward manner. This dense connectivity enhances feature reuse and 
facilitates gradient flow throughout the network, leading to improved 
parameter efficiency and feature propagation. As a result, DenseNet has shown 
significant improvements in performance and efficiency compared to earlier 
architectures.
Finally, EfficientNet \cite{efficientnet} proposed a new scaling method that uniformly scales 
depth, width, and resolution leading to a better performance. It was able to 
achieve state of the art results on ImageNet as well as on CIFAR-100 while 
being 8.4 times smaller and 6.1 times faster on inference than the best models.

Through this section it was possible to compare main improvements between 
different CNN architectures, and to understand how they evolved over time.
However, by their nature, CNNs are design to extract features from images that 
can be associated to specific object patterns. Through some hyperparameters, 
like kernel size, downsampling, and number of layers, they can be adapted to extract 
features at different scales and with different resolutions. Considering scene 
understanding in driving scenarios, it is fundamental to caputre relative and 
absolute spatial relations of sub-areas in the main scene. 
Therefore, to address this problem, it is necessary to mode towards models 
designed with different architectures, capable to include these kind of 
information. One possible solution is using a vision transformer model (ViT)
\cite{vit}, that is based on the transformer architecture 
\cite{attention_is_all_you_need}. It consists of a 
self-attention mechanism that allows to tokenize sub-areas of the image and 
capture their spatial relations. This implies that relations are not just 
limited as local features, like with CNNs, but can be extended to all locations 
in the image through the tokens.

In Table \ref{tab:cnn_comparison} a comparison of the discussed models is 
reported. It is possible to see the trending increase of parameters and 
performance over time, but also how since Inception model size has been reduced 
thanks to the introduction of new techniques. It is also possible to notice 
how vision transformer achieves state of the art results with a large model 
size and computational cost, due to the connections between all tokens for 
the self-attention mechanism.

\begin{table}[ht]
    \centering
    \begin{tabular}{l|c|c|c|c|c|c}
        \hline
        \textbf{Model} & \textbf{top-1 [\%]} & \textbf{top-5 [\%]} & 
        \textbf{Params.} & \textbf{GFLOPS} & \textbf{Size [MB]} & 
        \textbf{Year} \\
        \hline
        \hline
        AlexNet & 56.522 & 79.066 & 61.1M & 0.71 & 233.1 & 2012 \\
        \hline
        VGG11 & 69.020 & 88.628 & 132.9M & 7.61 & 506.8 & 2014 \\
        VGG13 & 69.928 & 89.246 & 133.1M & 11.31 & 507.5 & \\
        VGG16 & 71.592 & 90.382 & 138.4M & 15.47 & 527.8 & \\
        VGG19 & 72.376 & 90.876 & 143.7M & 19.63 & 548.1 & \\
        \hline
        Inception\_v3 & 77.294 & 93.45 & 27.2M & 5.71 & 103.9 & 2014 \\
        \hline
        ResNet18 & 69.758 & 89.078 & 11.7M & 1.81 & 44.7 & 2015 \\
        ResNet34 & 73.314 & 91.420 & 21.8M & 3.66 & 83.3 & \\
        ResNet50 & 76.130 & 92.862 & 25.6M & 4.09 & 97.8 & \\
        ResNet101 & 77.374 & 93.546 & 44.6M & 7.8 & 170.5 & \\
        ResNet152 & 78.312 & 94.046 & 60.2M & 11.51 & 230.4 & \\
        \hline
        DenseNet121 & 74.434 & 91.972 & 7.9M & 2.83 & 30.8 & 2016 \\
        DenseNet161 & 77.138 & 93.560 & 28.7M & 7.73 & 110.4 & \\
        DenseNet169 & 75.600 & 92.806 & 14.2M & 3.36 & 54.7 & \\
        DenseNet201 & 76.896 & 93.370 & 20.1M & 4.29 & 77.4 & \\
        \hline
        Effic.Net\_b0 & 77.692 & 93.532 & 5.3M & 0.39 & 20.5 & 2019 \\
        Effic.Net\_b1 & 78.642 & 94.186 & 7.8M & 0.69 & 30.1 & \\
        Effic.Net\_b2 & 80.608 & 95.310 & 9.2M & 1.09 & 35.2 & \\
        Effic.Net\_b3 & 82.008 & 96.054 & 12.3M & 1.83 & 47.2 & \\
        Effic.Net\_b4 & 83.384 & 96.594 & 19.4M & 4.39 & 74.5 & \\
        Effic.Net\_b5 & 83.444 & 96.628 & 30.4M & 10.27 & 116.9 & \\
        Effic.Net\_b6 & 84.008 & 96.916 & 43.1M & 19.07 & 165.4 & \\
        Effic.Net\_b7 & 84.122 & 96.908 & 66.3M & 37.75 & 254.7 & \\
        \hline
        ViT-b\_16 & 81.072 & 95.318 & 86.6M & 17.56 & 330.3 & 2020 \\
        ViT-b\_32 & 75.912 & 92.466 & 88.3M & 4.41 & 336.6 & \\
        ViT-l\_16 & 79.662 & 94.638 & 304.4M & 61.55 & 1161.0 & \\
        ViT-l\_32 & 76.972 & 93.070 & 306.6M & 15.38 & 1169.4 & \\
        ViT-h\_14 & 88.552 & 98.694 & 633.5M & 1016.72 & 2416.6 & \\
        \hline
    \end{tabular}
    \caption{Comparison of vision models for image classification on ImageNet}
    \label{tab:cnn_comparison}
\end{table}


\section{Video Classification}
Along with image classification, time-series analysis started gaining attention. 
The initial focus was on univariate and multi-variate input 
data not directly related to images. 
The introduction of Recurrent Neural Networks (RNNs) \cite{rnn} marked a 
significant improvement in analyzing this kind of data. Recurrent neural 
network represents the simplest sequential architecture, and its main 
peculiarity is the presence of a feedback loop that allows to save an internal 
state based on information from previous time steps. Then for each current time 
step the input is combined with the internal state.
Despite the potential of RNNs, they are not able to capture long-term 
information from data sequences. This is due to the vanishing and exploding 
gradient problem, when there are many feedback loops in the network, and it is 
similar to the problem described for deep CNNs.

To address this issue, Long Short-Term Memory (LSTM) \cite{lstm} was introduced. 
LSTM is a modified version of RNN capable to capture both long and short-term 
dependencies in data sequences through specific internal gates. 
A further improvement was introduced with Gated Recurrent Units (GRUs) 
\cite{gru}, that are a simplified version of LSTM with fewer gates and 
parameters.

However, with the advent of image classification in computer vision, 
the interest in video analysis rose. Considering the previous work on RNNs, 
it was natural to extend the concept of time-series analysis to video time 
frames combining RNNs with CNNs. This led to the introduction of 3D CNNs that 
are capable to extract spatio-temporal features from video sequences through 
3D convolutional kernels.

As for image classification, recent developments in the field focused on 
transformer-based models. In particular, the recent ViViT model \cite{vivit} 
utilizes a similar concept of 3D CNNs, but applied to the vision transformer. 
The model stacks images of the video sequence and then applies the transformer 
architecture to capture spatio-temporal relations between them. 
While this approach outperforms 3D CNNs in terms of effectiveness, it comes with 
increased computational costs for training. Additionally, it is required to 
tune numerous hyperparameters.

\section{\acl{vad}}
\ac{vad} is a challenging task that aims to 
identify patterns in data that do not conform to expected results. It is a 
more general task, not only focused in driving scenarios, but also used in 
medical imaging, surveillance systems, and industrial quality control.
However, many of these techniques are cross-domain, meaning they can be applied and 
adapted to various fields beyond their initial area of application.
Generally, the two main aspects to focus on are the availability 
of datasets and the development of effective models. In the following subsections
we will list some of the most common datasets and models used in the field of 
\ac{vad}. We also distinguish training methods, supervised, unsupervised and 
semi-supervised, to accomplish different tasks.

However, \ac{vad} is usually performed in an unsupervised mode due to the 
scarcity of data. Therefore, when necessary, we will also include some works 
related to \ac{var} that can be adapted to \ac{vad}.

\subsection{Existing \acs{vad} Datasets}
Considering the common techniques used in \ac{vad}, and the initial focus 
on surveillance systems, many more datasets have been released in this field 
compared to driving scenarios. Therefore, we divide the datasets into 
two categories: surveillance and driving-related datasets.

\subsubsection*{Surveillance Datasets}
Li et al. proposed UCSD Ped1/Ped2 \cite{ucsd}, 
a dataset of densely 
crowded pedestrian walkways to detect abnormal movements and prohibited objects.
In particular, "Ped1" is based on 70 scenes where peple are walking toward and 
away from the camera.
"Ped2" is composed of 28 scenes depicting pedestrians walking almost 
horizontally to the camera.
Another dataset is Avenue \cite{avenue}, a dataset of 15 sequences with 14 
unusual events, such as throwing objects, running, and loitering, with a total 
of 35,240 frames.
Liu et al. proposed ShanghaiTech \cite{shanghaitech_dataset}, a dataset of 437 
videos (330 
for training and 107 for testing) with 130 abnormal events and a total of 
13 different scenes related to walking pedestrians.
Finally, the UCF-Crime dataset \cite{ucf_crime} is composed of 1900 videos 
with 13 anomalies including robberies, accidents, and thefts.

\subsubsection*{Driving Datasets}
Chan et al. introduced a new dataset with 678 dashcam accident videos taken from 
the web \cite{anticip_accident_dashcam}; in each video, the last 10 frames are 
labelled as anomalous.
Yao et al. proposed AnAn Accident Detection (A3D)\cite{a3d}, which contains 1,500
anomalous driving videos with the start and end frames' annotations. In total 
the dataset contains, respctively, 79,991 and 128,175 training and testing frames.
There are 1,500 anomaly events that include targets like cars, trucks, bikes, 
pedestrians and animals.

Considering the interaction of the driver's gaze towards the environment,
there is Driver Attention Prediction in Driving Accident scenarios (DADA) 
\cite{dada1}, \cite{dada2}, a dataset of 2,000 videos focused on the prediction 
of the driver's gaze in accident scenarios. In total they collected 658,476 
frames, each labelled as normal or anomalous. A similar work was proposed 
by Palazzi et al. with DR(eye)VE \cite{dreyeve}, a dataset based on 74 
videos, 8 drivers, over 500,000 frames, 6 total hours of driving in 
downtowns, countrysides and highways. 
However, DR(eye)VE is focused on prediction of the driver's gaze in 
safe scenarios, not in accident ones.

Xia et al. created the BDD-A \cite{bdd_a} dataset, a subset of the Berkeley 
DeepDrive dataset \cite{bdd100k} with 1,232 critical scenes and a total of 
3.5 hours of driving. To create the dataset, they selected video clips that 
both included braking events and took place in busy areas, and then included 
6.5 seconds before and 3.5 seconds after the braking event.
They also collected gaze data of 45 partecipants, who were asked to watch 
the videos and press a button when they noticed an unsafe maneuver.

Finally, a recent dataset focused on detection of traffic accidents is DoTA 
\cite{dota_dataset}. It contains 4,677 videos with temporal, spatial, and 
categorical annotations. In total there are 9 different categories of accidents,
including collision with other vehicles in different directions and out-of-control 
situations. Moreover, they double the classes, splitting each category to 
ego-involved and non-ego-involved accidents, resulting in 18 categories total.

\subsection{State-of-the-Art \acs{vad} and \acs{var} Models}
\acl{vad} models for driving scenarios can be divide into three main categories, 
depending on the available data and learning approach: 
unsupervised, semi-supervised, and supervised mode.
Typically, it is easy to collect unlabelled data through ego-vehicle-mounted 
cameras or dashcams, while labelled data is much more difficult and expensive 
to obtain. Moreover, the quality of the labels can be affected by human 
subjectivity, age, and experience. Therefore, most of the research in this 
field is focused on unsupervised and semi-supervised learning.

Another imoprtant division is related to what features to extract: some models 
consider a single frame to analyze, while others take a sequence through a 
sliding time window. 

\subsubsection*{Unsupervised Learning}
Hasan et al. \cite{hasan_convae} proposed a \ac{convae} model for reconstructing 
some stacked frames. Then anomalies were identified by comparing the 
reconstruction error. They fonud out that anomalies were better detected with 
longer sequences. To process the sequence with a \ac{cnn}, they input the data 
in a temporal cuboid format, converting RGB images to grayscale.

Medel et al. \cite{medel_lstmconvae} and Luo et al. \cite{luo_lstmconvae}
 used a spatio-temporal architecture, a 
combination of a \ac{cnn} to extract spatial features for each frame and a 
\ac{lstm} to capture temporal dependencies between all the features. 
The model is also called \ac{convlstmae}.
The end 
goal is also to reconstruct a sequence of frames, and anomalies are detected 
by comparing the reconstruction error. However, they tested the model on 
surveillance datasets, including Avenue \cite{a3d} and UCSD \cite{ucsd}.

Regarding driving scenarios, Yao et al. \cite{dota_dataset} introduced a 
\emph{when-where-what} pipeline to detect, localize and recognize anomalous 
events from their custom dataset (DoTA). For the reconstruction task in 
unsupervised mode, they compare five different models described before: 
\ac{convae} \cite{hasan_convae}, \ac{convlstmae} \cite{luo_lstmconvae}, 
AnoPred \cite{shanghaitech_dataset}, TAD \cite{a3d}, 

\subsubsection*{Supervised Learning}
\ac{vad} problems can be addressed through supervised learning in different ways.
On one hand, the model can be trained to make a binary classification (anomaly 
or not), or to detect the specific type of anomaly. The approach depends on the 
availability of data and the specific problem to solve.

A 3D \ac{cnn} architecture was proposed by 
\cites{tran_3dconvnet}{carreira_3dconvnet}{diba_3dconvnet},
for capturing spatio-temporal
features from video sequences. For the action recognition task, they added a 
linear layer on top of the network.

Gao et al. \cite{gao_lstm_encoder} used a \ac{lstm} encoder-decoder architecture 
to predict future frames in a video sequence, like in the unsupervised 
approach. However, they also added a classification layer to predict the 
future action, depending on the ouptut predicted in the decoder stage.

Finally, Arnab et al. \cite{vivit} proposed a pure-transformer based model for 
video classification. The model is called ViViT and it is able to capture 
spatio-temporal features without sequential stages.
Also Huang et al. \cite{vivit} used the same architecture, better analyzing how 
quality of data source, data preprocessing and augmentation affect the results.

\subsubsection*{Semi-Supervised Learning}
Semi-supervised learning uses a small amount of labelled data and a large 
quanitty of unlabelled samples. However, there are different approaches to 
manipulate unsupervised data to improve the model's performance.

Shou et al. \cite{shou_gan} trained a 3D \ac{cnn} to detect starting windows 
of specific actions. Moreover, they used a \ac{gan} to generate new frames 
that are difficult to distinguish and they do not correspond to starting actions.
Those frames are called \emph{hard negatives} by the authors.

Xu et al. \cite{xu_pl_var} used a pseudo-labelling method to improve video 
action recognition with a 3D \ac{cnn}. They introduced an auxiliary lightweight 
model in addition to the backbone, to predict pseudo-labels for each other.

Zeng et al. \cite{zenc_enc_dec_ssl} used an unique encoder-decoder architecture 
to reconstruct both labelled and unlabelled video sequences.
Moreover, for labelled data, there is a classification layer to predict the 
action label.